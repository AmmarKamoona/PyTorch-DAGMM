{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from train import TrainerDAGMM\n",
    "from test import eval\n",
    "from preprocess import get_KDDCup99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1024/317394: [>...............................] - ETA 0.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/PyTorch-DAGMM/forward_step.py:78: UserWarning: torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next release. Please use torch.cholesky instead and note that the :attr:`upper` argument in torch.cholesky defaults to ``False``.\n",
      "  l = torch.potrf(a, False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317394/317394: [===============================>] - ETA 1.2sss\n",
      "Training DAGMM... Epoch: 0, Loss: 1607.736\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 1, Loss: 1606.200\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 2, Loss: 1603.944\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 3, Loss: 1603.360\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 4, Loss: 1602.270\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 5, Loss: 1603.221\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 6, Loss: 1602.277\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 7, Loss: 1602.703\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 8, Loss: 1610.484\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 9, Loss: 1606.915\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 10, Loss: 1601.540\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 11, Loss: 1604.254\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 12, Loss: 1603.117\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 13, Loss: 1606.571\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 14, Loss: 1604.907\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 15, Loss: 1602.035\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 16, Loss: 1603.317\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 17, Loss: 1604.918\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 18, Loss: 1601.183\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 19, Loss: 1607.493\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 20, Loss: 1604.198\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 21, Loss: 1604.835\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 22, Loss: 1603.895\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 23, Loss: 1602.741\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 24, Loss: 1604.658\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 25, Loss: 1603.342\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 26, Loss: 1605.463\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 27, Loss: 1607.967\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 28, Loss: 1604.046\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 29, Loss: 1601.095\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 30, Loss: 1602.402\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 31, Loss: 1605.229\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 32, Loss: 1606.366\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 33, Loss: 1606.789\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 34, Loss: 1601.269\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 35, Loss: 1603.696\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 36, Loss: 1603.566\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 37, Loss: 1605.645\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 38, Loss: 1607.471\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 39, Loss: 1602.533\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 40, Loss: 1605.414\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 41, Loss: 1603.244\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 42, Loss: 1609.072\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 43, Loss: 1606.707\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 44, Loss: 1604.101\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 45, Loss: 1602.071\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 46, Loss: 1604.740\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 47, Loss: 1606.205\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 48, Loss: 1606.077\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 49, Loss: 1602.333\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 50, Loss: 1603.859\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 51, Loss: 1602.930\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 52, Loss: 1606.925\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 53, Loss: 1603.042\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 54, Loss: 1605.475\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 55, Loss: 1605.998\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 56, Loss: 1607.876\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 57, Loss: 1603.338\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 58, Loss: 1602.711\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 59, Loss: 1603.669\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 60, Loss: 1608.301\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 61, Loss: 1602.635\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 62, Loss: 1602.669\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 63, Loss: 1604.118\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 64, Loss: 1605.608\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 65, Loss: 1607.677\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 66, Loss: 1607.727\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 67, Loss: 1604.764\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 68, Loss: 1607.547\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 69, Loss: 1602.167\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 70, Loss: 1604.996\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 71, Loss: 1604.680\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 72, Loss: 1605.776\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 73, Loss: 1606.901\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 74, Loss: 1607.970\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 75, Loss: 1605.507\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 76, Loss: 1604.118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 77, Loss: 1604.082\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 78, Loss: 1602.607\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 79, Loss: 1604.595\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 80, Loss: 1606.174\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 81, Loss: 1602.784\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 82, Loss: 1603.345\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 83, Loss: 1605.809\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 84, Loss: 1606.218\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 85, Loss: 1603.407\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 86, Loss: 1603.408\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 87, Loss: 1607.578\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 88, Loss: 1602.462\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 89, Loss: 1604.116\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 90, Loss: 1603.602\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 91, Loss: 1604.127\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 92, Loss: 1604.901\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 93, Loss: 1603.909\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 94, Loss: 1606.228\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 95, Loss: 1607.782\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 96, Loss: 1607.836\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 97, Loss: 1602.437\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 98, Loss: 1607.960\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 99, Loss: 1603.760\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 100, Loss: 1604.475\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 101, Loss: 1603.117\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 102, Loss: 1603.968\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 103, Loss: 1604.202\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 104, Loss: 1605.843\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 105, Loss: 1602.361\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 106, Loss: 1603.979\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 107, Loss: 1607.401\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 108, Loss: 1605.322\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 109, Loss: 1607.550\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 110, Loss: 1608.693\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 111, Loss: 1604.396\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 112, Loss: 1608.442\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 113, Loss: 1604.327\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 114, Loss: 1604.977\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 115, Loss: 1604.842\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 116, Loss: 1601.356\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 117, Loss: 1603.287\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 118, Loss: 1600.990\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 119, Loss: 1603.926\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 120, Loss: 1601.096\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 121, Loss: 1603.360\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 122, Loss: 1603.032\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 123, Loss: 1604.018\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 124, Loss: 1607.300\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 125, Loss: 1604.375\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 126, Loss: 1605.258\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 127, Loss: 1604.062\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 128, Loss: 1604.700\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 129, Loss: 1603.765\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 130, Loss: 1605.865\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 131, Loss: 1604.946\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 132, Loss: 1604.156\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 133, Loss: 1606.284\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 134, Loss: 1604.020\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 135, Loss: 1603.693\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 136, Loss: 1604.048\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 137, Loss: 1606.777\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 138, Loss: 1603.900\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 139, Loss: 1602.819\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 140, Loss: 1606.617\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 141, Loss: 1604.789\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 142, Loss: 1606.046\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 143, Loss: 1604.859\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 144, Loss: 1604.437\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 145, Loss: 1603.444\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 146, Loss: 1601.347\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 147, Loss: 1604.389\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 148, Loss: 1604.957\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 149, Loss: 1602.920\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 150, Loss: 1603.931\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 151, Loss: 1608.511\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 152, Loss: 1603.776\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 153, Loss: 1604.690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 154, Loss: 1601.188\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 155, Loss: 1604.389\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 156, Loss: 1604.972\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 157, Loss: 1606.877\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 158, Loss: 1604.394\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 159, Loss: 1605.878\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 160, Loss: 1606.166\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 161, Loss: 1605.273\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 162, Loss: 1607.440\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 163, Loss: 1603.969\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 164, Loss: 1604.351\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 165, Loss: 1602.479\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 166, Loss: 1604.259\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 167, Loss: 1601.140\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 168, Loss: 1603.008\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 169, Loss: 1600.260\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 170, Loss: 1602.954\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 171, Loss: 1605.036\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 172, Loss: 1600.865\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 173, Loss: 1603.610\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 174, Loss: 1607.597\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 175, Loss: 1605.097\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 176, Loss: 1606.224\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 177, Loss: 1606.523\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 178, Loss: 1606.731\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 179, Loss: 1603.295\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 180, Loss: 1606.411\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 181, Loss: 1603.119\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 182, Loss: 1603.117\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 183, Loss: 1603.769\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 184, Loss: 1605.163\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 185, Loss: 1605.318\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 186, Loss: 1603.020\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 187, Loss: 1603.931\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 188, Loss: 1607.167\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 189, Loss: 1602.931\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 190, Loss: 1603.545\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 191, Loss: 1601.197\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 192, Loss: 1603.904\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 193, Loss: 1603.827\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 194, Loss: 1601.788\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 195, Loss: 1605.877\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 196, Loss: 1606.279\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 197, Loss: 1604.938\n",
      "317394/317394: [===============================>] - ETA 0.0ss\n",
      "Training DAGMM... Epoch: 198, Loss: 1607.383\n",
      "317394/317394: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 199, Loss: 1605.662\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    num_epochs=200\n",
    "    patience=50\n",
    "    lr=1e-4\n",
    "    lr_milestones=[50]\n",
    "    batch_size=1024\n",
    "    latent_dim=1\n",
    "    n_gmm=4\n",
    "    lambda_energy=0.1\n",
    "    lambda_cov=0.005\n",
    "    \n",
    "    \n",
    "args = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = get_KDDCup99(args)\n",
    "\n",
    "dagmm = TrainerDAGMM(args, data, device)\n",
    "dagmm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:88: UserWarning: torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next release. Please use torch.cholesky instead and note that the :attr:`upper` argument in torch.cholesky defaults to ``False``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.7816, Recall : 0.7400, F-score : 0.7602\n",
      "ROC AUC score: 0.941\n"
     ]
    }
   ],
   "source": [
    "from test import eval\n",
    "\n",
    "labels, scores = eval(dagmm.model, data, device, args.n_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "scores_in = scores[np.where(labels==0)[0]]\n",
    "scores_out = scores[np.where(labels==1)[0]]\n",
    "\n",
    "\n",
    "in_ = pd.DataFrame(scores_in, columns=['Inlier'])\n",
    "out_ = pd.DataFrame(scores_out, columns=['Outlier'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "in_.plot.kde(ax=ax, legend=True, title='Outliers vs Inliers (Deep SVDD)')\n",
    "out_.plot.kde(ax=ax, legend=True)\n",
    "#plt.xlim(-0.05, 0.08)\n",
    "ax.grid(axis='x')\n",
    "ax.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
