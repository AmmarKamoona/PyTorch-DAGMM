{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from train import TrainerDAGMM\n",
    "from test import eval\n",
    "from preprocess import get_KDDCup99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1024/198371: [>...............................] - ETA 0.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/PyTorch-DAGMM/forward_step.py:79: UserWarning: torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next release. Please use torch.cholesky instead and note that the :attr:`upper` argument in torch.cholesky defaults to ``False``.\n",
      "  l = torch.potrf(a, False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198371/198371: [===============================>] - ETA 1.2sss\n",
      "Training DAGMM... Epoch: 0, Loss: 43065038.330\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 1, Loss: 43097073.753\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 2, Loss: 43083459.608\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 3, Loss: 43092447.732\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 4, Loss: 43082362.928\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 5, Loss: 43072141.938\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 6, Loss: 43085888.639\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 7, Loss: 43078235.979\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 8, Loss: 43077461.897\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 9, Loss: 43084908.165\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 10, Loss: 43068323.010\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 11, Loss: 43106304.041\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 12, Loss: 43076459.814\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 13, Loss: 43081505.381\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 14, Loss: 43088439.196\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 15, Loss: 43070971.918\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 16, Loss: 43065649.526\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 17, Loss: 43083100.598\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 18, Loss: 43069283.175\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 19, Loss: 43075103.320\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 20, Loss: 43080082.680\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 21, Loss: 43076174.887\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 22, Loss: 43088163.196\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 23, Loss: 43078780.619\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 24, Loss: 43084651.443\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 25, Loss: 43072749.897\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 26, Loss: 43080775.155\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 27, Loss: 43085080.990\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 28, Loss: 43076550.062\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 29, Loss: 43076109.031\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 30, Loss: 43094993.258\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 31, Loss: 43080644.041\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 32, Loss: 43100788.144\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 33, Loss: 43084281.629\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 34, Loss: 43077232.701\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 35, Loss: 43085815.979\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 36, Loss: 43086250.928\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 37, Loss: 43088283.979\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 38, Loss: 43074096.227\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 39, Loss: 43074791.814\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 40, Loss: 43095966.680\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 41, Loss: 43101322.536\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 42, Loss: 43103282.680\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 43, Loss: 43102434.845\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 44, Loss: 43069060.474\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 45, Loss: 43074877.072\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 46, Loss: 43081692.969\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 47, Loss: 43085332.990\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 48, Loss: 43077506.701\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 49, Loss: 43078397.175\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 50, Loss: 43095240.474\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 51, Loss: 43088525.773\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 52, Loss: 43081633.443\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 53, Loss: 43084432.907\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 54, Loss: 43077313.010\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 55, Loss: 43062447.072\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 56, Loss: 43082435.052\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 57, Loss: 43080778.289\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 58, Loss: 43096253.072\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 59, Loss: 43082393.526\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 60, Loss: 43066855.732\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 61, Loss: 43082993.773\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 62, Loss: 43069702.206\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 63, Loss: 43095562.680\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 64, Loss: 43085220.474\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 65, Loss: 43095874.124\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 66, Loss: 43068457.546\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 67, Loss: 43063635.072\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 68, Loss: 43101546.598\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 69, Loss: 43079401.443\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 70, Loss: 43095334.680\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 71, Loss: 43069860.082\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 72, Loss: 43064034.742\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 73, Loss: 43062107.052\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 74, Loss: 43090735.959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 75, Loss: 43073175.381\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 76, Loss: 43081122.103\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 77, Loss: 43087262.392\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 78, Loss: 43067343.423\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 79, Loss: 43103929.918\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 80, Loss: 43089660.371\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 81, Loss: 43080217.258\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 82, Loss: 43090317.237\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 83, Loss: 43078276.103\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 84, Loss: 43080412.454\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 85, Loss: 43082480.907\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 86, Loss: 43081265.155\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 87, Loss: 43069174.186\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 88, Loss: 43070057.691\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 89, Loss: 43082439.340\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 90, Loss: 43075413.216\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 91, Loss: 43090430.206\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 92, Loss: 43073045.670\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 93, Loss: 43076483.856\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 94, Loss: 43079612.247\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 95, Loss: 43063771.897\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 96, Loss: 43102942.433\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 97, Loss: 43077618.309\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 98, Loss: 43094660.412\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 99, Loss: 43079660.598\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 100, Loss: 43082306.330\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 101, Loss: 43083539.608\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 102, Loss: 43078700.536\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 103, Loss: 43106430.041\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 104, Loss: 43076232.515\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 105, Loss: 43087332.227\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 106, Loss: 43070379.938\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 107, Loss: 43072816.928\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 108, Loss: 43079754.928\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 109, Loss: 43090714.515\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 110, Loss: 43067471.485\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 111, Loss: 43089620.887\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 112, Loss: 43091823.361\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 113, Loss: 43073657.567\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 114, Loss: 43091895.010\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 115, Loss: 43086023.299\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 116, Loss: 43083342.969\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 117, Loss: 43078794.887\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 118, Loss: 43067318.660\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 119, Loss: 43079388.082\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 120, Loss: 43063478.124\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 121, Loss: 43088449.485\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 122, Loss: 43072771.794\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 123, Loss: 43068286.969\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 124, Loss: 43080752.969\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 125, Loss: 43085541.010\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 126, Loss: 43064572.536\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 127, Loss: 43082481.052\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 128, Loss: 43080291.979\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 129, Loss: 43085915.010\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 130, Loss: 43076429.814\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 131, Loss: 43057598.021\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 132, Loss: 43077852.866\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 133, Loss: 43084148.722\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 134, Loss: 43073377.423\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 135, Loss: 43081267.567\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 136, Loss: 43084593.299\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 137, Loss: 43088257.546\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 138, Loss: 43073273.629\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 139, Loss: 43074540.062\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 140, Loss: 43071295.794\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 141, Loss: 43080391.526\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 142, Loss: 43070808.289\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 143, Loss: 43086532.082\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 144, Loss: 43075820.680\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 145, Loss: 43075231.134\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 146, Loss: 43084941.588\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 147, Loss: 43074511.546\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 148, Loss: 43079230.330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 149, Loss: 43075810.515\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 150, Loss: 43075883.320\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 151, Loss: 43090732.660\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 152, Loss: 43079805.670\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 153, Loss: 43081801.773\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 154, Loss: 43071096.763\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 155, Loss: 43078624.433\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 156, Loss: 43072408.371\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 157, Loss: 43080850.144\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 158, Loss: 43088534.433\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 159, Loss: 43095102.660\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 160, Loss: 43076601.629\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 161, Loss: 43086146.577\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 162, Loss: 43092120.103\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 163, Loss: 43073036.082\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 164, Loss: 43081610.948\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 165, Loss: 43074607.381\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 166, Loss: 43091955.258\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 167, Loss: 43063851.010\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 168, Loss: 43062664.412\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 169, Loss: 43084274.907\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 170, Loss: 43082829.237\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 171, Loss: 43074880.907\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 172, Loss: 43079505.052\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 173, Loss: 43090372.928\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 174, Loss: 43073833.670\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 175, Loss: 43066475.732\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 176, Loss: 43061693.794\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 177, Loss: 43083229.093\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 178, Loss: 43084695.175\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 179, Loss: 43075621.464\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 180, Loss: 43077137.773\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 181, Loss: 43082469.216\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 182, Loss: 43081204.969\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 183, Loss: 43075331.340\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 184, Loss: 43085729.072\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 185, Loss: 43074187.835\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 186, Loss: 43082189.835\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 187, Loss: 43095910.392\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 188, Loss: 43086527.010\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 189, Loss: 43080836.763\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 190, Loss: 43090150.124\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 191, Loss: 43073106.825\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 192, Loss: 43092514.392\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 193, Loss: 43098151.794\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 194, Loss: 43066148.577\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 195, Loss: 43091540.639\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 196, Loss: 43086788.845\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 197, Loss: 43085903.072\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 198, Loss: 43081863.134\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 199, Loss: 43098706.041\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    num_epochs=200\n",
    "    patience=50\n",
    "    lr=1e-4\n",
    "    lr_milestones=[50]\n",
    "    batch_size=1024\n",
    "    latent_dim=1\n",
    "    n_gmm=4\n",
    "    lambda_energy=0.1\n",
    "    lambda_cov=0.005\n",
    "    \n",
    "    \n",
    "args = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = get_KDDCup99(args)\n",
    "\n",
    "dagmm = TrainerDAGMM(args, data, device)\n",
    "dagmm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/PyTorch-DAGMM/forward_step.py:79: UserWarning: torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next release. Please use torch.cholesky instead and note that the :attr:`upper` argument in torch.cholesky defaults to ``False``.\n",
      "  l = torch.potrf(a, False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.9486, Recall : 0.9169, F-score : 0.9325\n",
      "ROC AUC score: 98.00\n"
     ]
    }
   ],
   "source": [
    "from test import eval\n",
    "\n",
    "labels, scores = eval(dagmm.model, data, device, args.n_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "scores_in = scores[np.where(labels==0)[0]]\n",
    "scores_out = scores[np.where(labels==1)[0]]\n",
    "\n",
    "\n",
    "in_ = pd.DataFrame(scores_in, columns=['Inlier'])\n",
    "out_ = pd.DataFrame(scores_out, columns=['Outlier'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "in_.plot.kde(ax=ax, legend=True, title='Outliers vs Inliers (Deep SVDD)')\n",
    "out_.plot.kde(ax=ax, legend=True)\n",
    "ax.grid(axis='x')\n",
    "ax.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
