{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from train import TrainerDAGMM\n",
    "from test import eval\n",
    "from preprocess import get_KDDCup99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1024/198371: [>...............................] - ETA 0.0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/PyTorch-DAGMM/forward_step.py:79: UserWarning: torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next release. Please use torch.cholesky instead and note that the :attr:`upper` argument in torch.cholesky defaults to ``False``.\n",
      "  l = torch.potrf(a, False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198371/198371: [===============================>] - ETA 1.5sss\n",
      "Training DAGMM... Epoch: 0, Loss: 468.669\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 1, Loss: 468.159\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 2, Loss: 468.113\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 3, Loss: 468.272\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 4, Loss: 468.354\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 5, Loss: 468.405\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 6, Loss: 468.411\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 7, Loss: 468.272\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 8, Loss: 468.076\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 9, Loss: 468.322\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 10, Loss: 468.557\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 11, Loss: 468.372\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 12, Loss: 468.420\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 13, Loss: 468.238\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 14, Loss: 468.649\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 15, Loss: 468.604\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 16, Loss: 467.818\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 17, Loss: 468.238\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 18, Loss: 468.620\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 19, Loss: 468.351\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 20, Loss: 468.298\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 21, Loss: 468.286\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 22, Loss: 468.644\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 23, Loss: 468.203\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 24, Loss: 468.981\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 25, Loss: 468.701\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 26, Loss: 468.420\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 27, Loss: 468.565\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 28, Loss: 468.367\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 29, Loss: 468.682\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 30, Loss: 468.960\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 31, Loss: 468.454\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 32, Loss: 468.578\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 33, Loss: 468.067\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 34, Loss: 468.483\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 35, Loss: 468.072\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 36, Loss: 468.009\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 37, Loss: 468.519\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 38, Loss: 468.081\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 39, Loss: 467.981\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 40, Loss: 468.229\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 41, Loss: 468.438\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 42, Loss: 468.567\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 43, Loss: 467.890\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 44, Loss: 468.065\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 45, Loss: 469.046\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 46, Loss: 468.510\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 47, Loss: 468.440\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 48, Loss: 468.493\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 49, Loss: 468.211\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 50, Loss: 468.478\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 51, Loss: 468.127\n",
      "198371/198371: [===============================>] - ETA 0.1s\n",
      "Training DAGMM... Epoch: 52, Loss: 468.170\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 53, Loss: 468.165\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 54, Loss: 467.961\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 55, Loss: 468.193\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 56, Loss: 468.235\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 57, Loss: 468.480\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 58, Loss: 468.236\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 59, Loss: 468.607\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 60, Loss: 468.303\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 61, Loss: 468.391\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 62, Loss: 467.700\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 63, Loss: 468.724\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 64, Loss: 468.030\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 65, Loss: 468.782\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 66, Loss: 467.946\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 67, Loss: 468.219\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 68, Loss: 468.347\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 69, Loss: 468.322\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 70, Loss: 468.582\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 71, Loss: 468.470\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 72, Loss: 468.390\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 73, Loss: 467.993\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 74, Loss: 468.083\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 75, Loss: 468.663\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 76, Loss: 468.391\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 77, Loss: 467.732\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 78, Loss: 468.102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 79, Loss: 468.549\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 80, Loss: 468.255\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 81, Loss: 468.694\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 82, Loss: 468.468\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 83, Loss: 468.266\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 84, Loss: 468.694\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 85, Loss: 468.691\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 86, Loss: 468.050\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 87, Loss: 468.381\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 88, Loss: 468.115\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 89, Loss: 467.626\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 90, Loss: 468.444\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 91, Loss: 467.869\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 92, Loss: 468.464\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 93, Loss: 468.574\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 94, Loss: 468.095\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 95, Loss: 468.354\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 96, Loss: 468.051\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 97, Loss: 467.970\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 98, Loss: 468.215\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 99, Loss: 468.053\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 100, Loss: 467.596\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 101, Loss: 467.919\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 102, Loss: 467.943\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 103, Loss: 467.719\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 104, Loss: 467.999\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 105, Loss: 468.335\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 106, Loss: 468.364\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 107, Loss: 468.235\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 108, Loss: 468.134\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 109, Loss: 468.333\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 110, Loss: 468.319\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 111, Loss: 468.568\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 112, Loss: 468.201\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 113, Loss: 468.145\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 114, Loss: 468.636\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 115, Loss: 468.270\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 116, Loss: 468.132\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 117, Loss: 468.394\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 118, Loss: 468.679\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 119, Loss: 468.776\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 120, Loss: 468.494\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 121, Loss: 467.929\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 122, Loss: 468.629\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 123, Loss: 468.555\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 124, Loss: 468.480\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 125, Loss: 468.160\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 126, Loss: 468.496\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 127, Loss: 467.860\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 128, Loss: 467.885\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 129, Loss: 468.456\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 130, Loss: 468.689\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 131, Loss: 468.334\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 132, Loss: 468.266\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 133, Loss: 468.154\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 134, Loss: 467.977\n",
      "198371/198371: [===============================>] - ETA 0.1ss\n",
      "Training DAGMM... Epoch: 135, Loss: 468.037\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 136, Loss: 468.175\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 137, Loss: 467.948\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 138, Loss: 468.796\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 139, Loss: 468.232\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 140, Loss: 467.941\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 141, Loss: 468.591\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 142, Loss: 468.400\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 143, Loss: 468.445\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 144, Loss: 468.554\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 145, Loss: 468.329\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 146, Loss: 468.448\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 147, Loss: 468.330\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 148, Loss: 468.634\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 149, Loss: 468.276\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 150, Loss: 468.005\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 151, Loss: 468.467\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 152, Loss: 468.544\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 153, Loss: 468.501\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 154, Loss: 467.933\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 155, Loss: 468.274\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 156, Loss: 468.127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 157, Loss: 468.324\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 158, Loss: 468.759\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 159, Loss: 468.145\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 160, Loss: 467.670\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 161, Loss: 468.546\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 162, Loss: 468.632\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 163, Loss: 468.263\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 164, Loss: 468.282\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 165, Loss: 468.636\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 166, Loss: 468.717\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 167, Loss: 468.830\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 168, Loss: 468.616\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 169, Loss: 468.121\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 170, Loss: 468.289\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 171, Loss: 468.305\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 172, Loss: 468.567\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 173, Loss: 468.598\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 174, Loss: 468.507\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 175, Loss: 468.504\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 176, Loss: 468.364\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 177, Loss: 468.700\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 178, Loss: 467.783\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 179, Loss: 468.097\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 180, Loss: 467.739\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 181, Loss: 467.863\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 182, Loss: 467.962\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 183, Loss: 468.310\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 184, Loss: 468.360\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 185, Loss: 467.984\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 186, Loss: 468.329\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 187, Loss: 468.472\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 188, Loss: 468.513\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 189, Loss: 468.248\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 190, Loss: 468.399\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 191, Loss: 468.710\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 192, Loss: 468.799\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 193, Loss: 468.352\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 194, Loss: 468.163\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 195, Loss: 468.535\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 196, Loss: 468.003\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 197, Loss: 468.144\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 198, Loss: 468.214\n",
      "198371/198371: [===============================>] - ETA 0.0s\n",
      "Training DAGMM... Epoch: 199, Loss: 468.314\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    num_epochs=200\n",
    "    patience=50\n",
    "    lr=1e-4\n",
    "    lr_milestones=[50]\n",
    "    batch_size=1024\n",
    "    latent_dim=1\n",
    "    n_gmm=4\n",
    "    lambda_energy=0.1\n",
    "    lambda_cov=0.005\n",
    "    \n",
    "    \n",
    "args = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = get_KDDCup99(args)\n",
    "\n",
    "dagmm = TrainerDAGMM(args, data, device)\n",
    "dagmm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/PyTorch-DAGMM/forward_step.py:79: UserWarning: torch.potrf is deprecated in favour of torch.cholesky and will be removed in the next release. Please use torch.cholesky instead and note that the :attr:`upper` argument in torch.cholesky defaults to ``False``.\n",
      "  l = torch.potrf(a, False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.9595, Recall : 0.9359, F-score : 0.9476\n",
      "ROC AUC score: 0.989\n"
     ]
    }
   ],
   "source": [
    "from test import eval\n",
    "\n",
    "labels, scores = eval(dagmm.model, data, device, args.n_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "scores_in = scores[np.where(labels==0)[0]]\n",
    "scores_out = scores[np.where(labels==1)[0]]\n",
    "\n",
    "\n",
    "in_ = pd.DataFrame(scores_in, columns=['Inlier'])\n",
    "out_ = pd.DataFrame(scores_out, columns=['Outlier'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "in_.plot.kde(ax=ax, legend=True, title='Outliers vs Inliers (Deep SVDD)')\n",
    "out_.plot.kde(ax=ax, legend=True)\n",
    "ax.grid(axis='x')\n",
    "ax.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
